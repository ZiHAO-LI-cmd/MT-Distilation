{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer, MarianConfig\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import log_softmax, softmax\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Helsinki-NLP/opus-mt-tc-big-en-zle'\n",
    "teacher_model = MarianMTModel.from_pretrained(model_name)\n",
    "teacher_model.to(device)\n",
    "teacher_model.eval()\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianConfig {\n",
       "  \"_name_or_path\": \"Helsinki-NLP/opus-mt-tc-big-en-zle\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"relu\",\n",
       "  \"architectures\": [\n",
       "    \"MarianMTModel\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bad_words_ids\": [\n",
       "    [\n",
       "      61576\n",
       "    ]\n",
       "  ],\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 1024,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 4096,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 61576,\n",
       "  \"decoder_vocab_size\": 61577,\n",
       "  \"dropout\": 0.1,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 4096,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 25539,\n",
       "  \"forced_eos_token_id\": 25539,\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"max_length\": 512,\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"model_type\": \"marian\",\n",
       "  \"normalize_embedding\": false,\n",
       "  \"num_beams\": 4,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 61576,\n",
       "  \"scale_embedding\": true,\n",
       "  \"share_encoder_decoder_embeddings\": true,\n",
       "  \"static_position_embeddings\": true,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.40.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 61577\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = [line.strip() for line in file.readlines()]\n",
    "    return lines\n",
    "\n",
    "# 假设文件路径\n",
    "uk_file_path = './data/en-uk/NLLB.en-uk.uk'\n",
    "en_file_path = './data/en-uk/NLLB.en-uk.en'\n",
    "\n",
    "# 读取文件\n",
    "uk_sentences = read_text_file(uk_file_path)\n",
    "en_sentences = read_text_file(en_file_path)\n",
    "\n",
    "# 检查文件长度是否一致\n",
    "assert len(uk_sentences) == len(en_sentences), \"The number of sentences must be the same in both files.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_sentences, tgt_sentences, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.src_sentences = [\" >>ukr<< \" + sent for sent in src_sentences]\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.src_sentences[idx]\n",
    "        tgt_text = self.tgt_sentences[idx]\n",
    "\n",
    "        # 使用 tokenizer 进行编码\n",
    "        model_inputs = self.tokenizer(\n",
    "            text=src_text,\n",
    "            text_pair=tgt_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # 生成 decoder_input_ids\n",
    "        decoder_input_ids = self.tokenizer.encode(\n",
    "            tgt_text, \n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\", \n",
    "            return_tensors=\"pt\"\n",
    "        ).squeeze()\n",
    "\n",
    "        model_inputs['decoder_input_ids'] = decoder_input_ids\n",
    "        # 这里我们把 decoder_input_ids 也用作 labels\n",
    "        model_inputs['labels'] = decoder_input_ids.clone()\n",
    "\n",
    "        # 调整输入输出格式以去除批次维度\n",
    "        model_inputs = {key: val.squeeze(0) for key, val in model_inputs.items()}\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = TranslationDataset(en_sentences, uk_sentences, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(61577, 512, padding_idx=61576)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(61577, 512, padding_idx=61576)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(1024, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(61577, 512, padding_idx=61576)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(1024, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=61577, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从预训练模型加载配置\n",
    "teacher_config = MarianConfig.from_pretrained(\"Helsinki-NLP/opus-mt-tc-big-en-zle\")\n",
    "\n",
    "# 将配置转换为字典，修改参数，然后创建新的配置\n",
    "config_dict = teacher_config.to_dict()\n",
    "config_dict['num_hidden_layers'] = 3        # 减少层数\n",
    "config_dict['d_model'] = 512                # 减少隐藏层大小\n",
    "config_dict['decoder_attention_heads'] = 8  # 减少注意力头数量\n",
    "config_dict['encoder_attention_heads'] = 8  # 同上\n",
    "config_dict['decoder_ffn_dim'] = 2048       # 减少前馈网络维度\n",
    "config_dict['encoder_ffn_dim'] = 2048       # 同上\n",
    "\n",
    "# 创建新的配置对象\n",
    "student_config = MarianConfig(**config_dict)\n",
    "\n",
    "# 根据新配置初始化学生模型\n",
    "student_model = MarianMTModel(student_config)\n",
    "student_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianConfig {\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"relu\",\n",
       "  \"architectures\": [\n",
       "    \"MarianMTModel\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bad_words_ids\": [\n",
       "    [\n",
       "      61576\n",
       "    ]\n",
       "  ],\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 512,\n",
       "  \"decoder_attention_heads\": 8,\n",
       "  \"decoder_ffn_dim\": 2048,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 61576,\n",
       "  \"decoder_vocab_size\": 61577,\n",
       "  \"dropout\": 0.1,\n",
       "  \"encoder_attention_heads\": 8,\n",
       "  \"encoder_ffn_dim\": 2048,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 25539,\n",
       "  \"forced_eos_token_id\": 25539,\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"max_length\": 512,\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"model_type\": \"marian\",\n",
       "  \"normalize_embedding\": false,\n",
       "  \"num_beams\": 4,\n",
       "  \"num_hidden_layers\": 3,\n",
       "  \"pad_token_id\": 61576,\n",
       "  \"scale_embedding\": true,\n",
       "  \"share_encoder_decoder_embeddings\": true,\n",
       "  \"static_position_embeddings\": true,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.40.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 61577\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 4\n",
    "num_epochs = 3\n",
    "temperature = 5\n",
    "alpha = 0.5\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "optimizer = torch.optim.AdamW(student_model.parameters(), lr=learning_rate)\n",
    "\n",
    "def calculate_loss(student_outputs, teacher_outputs, labels):\n",
    "    s_logits = student_outputs.logits\n",
    "    t_logits = teacher_outputs.logits\n",
    "\n",
    "    vocab_size = s_logits.size(-1)\n",
    "    ce_logits = s_logits.view(-1, vocab_size)\n",
    "    ce_labels = labels.view(-1)\n",
    "    ce_loss = torch.nn.functional.cross_entropy(ce_logits, ce_labels)\n",
    "    student_log_probs = log_softmax(s_logits.view(-1, vocab_size) / temperature, dim=-1)\n",
    "    teacher_probs = softmax(t_logits.view(-1, vocab_size) / temperature, dim=-1)\n",
    "\n",
    "    distill_loss = torch.nn.functional.kl_div(student_log_probs, teacher_probs, reduction=\"batchmean\")\n",
    "    loss = (1 - alpha) * ce_loss + (alpha * temperature**2 / batch_size**2) * distill_loss\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    student_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 确保所有数据都转移到了适当的设备\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # 教师和学生模型的输出\n",
    "        teacher_outputs = teacher_model(input_ids=batch['input_ids'], attention_mask=batch.get('attention_mask'), decoder_input_ids=batch['decoder_input_ids'])\n",
    "        student_outputs = student_model(input_ids=batch['input_ids'], attention_mask=batch.get('attention_mask'), decoder_input_ids=batch['decoder_input_ids'])\n",
    "\n",
    "        # 计算损失并执行反向传播\n",
    "        loss = calculate_loss(student_outputs, teacher_outputs, batch['labels'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Average loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "student_model.save_pretrained(\"distilled-opus-mt-translation-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    # 对文本进行编码，将其转换为模型可以处理的格式\n",
    "    model_inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text):\n",
    "    # 对文本进行预处理\n",
    "    encoded_text = preprocess(text)\n",
    "    \n",
    "    # 生成翻译输出\n",
    "    translated_tokens = model.generate(**encoded_text)\n",
    "    \n",
    "    # 解码翻译结果\n",
    "    translated_text = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\n",
    "    return translated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['这是翻译的样本']\n"
     ]
    }
   ],
   "source": [
    "text = \">>zho<< This is a sample text for translation.\"\n",
    "translated_text = translate(text)\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"wmt16\", \"de-en\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/4548885 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4548885/4548885 [05:14<00:00, 14462.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(lambda x: {\"src_text\": x[\"translation\"][\"de\"], \"tgt_text\": x[\"translation\"][\"en\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation', 'src_text', 'tgt_text'],\n",
       "    num_rows: 4548885\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
